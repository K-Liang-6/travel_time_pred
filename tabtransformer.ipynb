{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"feature_eng_cat.csv\")\n",
    "new_data.drop([\"Unnamed: 0\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LEN', 'CALL_TYPE_A', 'CALL_TYPE_B', 'CALL_TYPE_C', 'DAY_TYPE_A',\n",
       "       'DAY_TYPE_B', 'DAY_TYPE_C', 'MISSING', 'CALL_A_DAY_A', 'CALL_A_DAY_B',\n",
       "       'CALL_A_DAY_C', 'CALL_B_DAY_A', 'CALL_B_DAY_B', 'CALL_B_DAY_C',\n",
       "       'MORNING', 'AFTERNOON', 'EVENING', 'NIGHT', 'S1', 'S2', 'S3', 'S4',\n",
       "       'HR_SIN', 'HR_COS', 'DAY_SIN', 'DAY_COS', 'WK_SIN', 'WK_COS', 'MON_SIN',\n",
       "       'MON_COS', 'YR_DUMMY', 'CALL_A_MORN', 'CALL_A_AFTER', 'CALL_A_EVE',\n",
       "       'CALL_A_NIT', 'CALL_B_MORN', 'CALL_B_AFTER', 'CALL_B_EVE', 'CALL_B_NIT',\n",
       "       'CALL_C_MORN', 'CALL_C_AFTER', 'CALL_C_EVE', 'CALL_C_NIT', 'CALL_A_S1',\n",
       "       'CALL_A_S2', 'CALL_A_S3', 'CALL_A_S4', 'CALL_B_S1', 'CALL_B_S2',\n",
       "       'CALL_B_S3', 'CALL_B_S4', 'CALL_C_S1', 'CALL_C_S2', 'CALL_C_S3',\n",
       "       'CALL_C_S4', 'DAY_A_MORN', 'DAY_A_AFTER', 'DAY_A_EVE', 'DAY_A_NIT',\n",
       "       'DAY_B_MORN', 'DAY_B_AFTER', 'DAY_B_EVE', 'DAY_B_NIT', 'DAY_C_MORN',\n",
       "       'DAY_C_AFTER', 'DAY_C_EVE', 'DAY_C_NIT', 'DAY_A_S1', 'DAY_A_S2',\n",
       "       'DAY_A_S3', 'DAY_A_S4', 'DAY_B_S1', 'DAY_B_S2', 'DAY_B_S3', 'DAY_B_S4',\n",
       "       'DAY_C_S1', 'DAY_C_S2', 'DAY_C_S3', 'DAY_C_S4', 'CALL_A_MISS',\n",
       "       'CALL_B_MISS', 'CALL_C_MISS', 'DAY_A_MISS', 'DAY_B_MISS', 'DAY_C_MISS',\n",
       "       'TAXI_ID_CAT', 'ORIGIN_CALL_CAT', 'ORIGIN_STAND_CAT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_data.drop(['LEN'],axis=1)\n",
    "y = new_data['LEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_names = ['CALL_TYPE_A', 'CALL_TYPE_B',\n",
    "       'CALL_TYPE_C', 'DAY_TYPE_A', 'DAY_TYPE_B', 'DAY_TYPE_C', 'MISSING',\n",
    "       'CALL_A_DAY_A', 'CALL_A_DAY_B', 'CALL_A_DAY_C', 'CALL_B_DAY_A',\n",
    "       'CALL_B_DAY_B', 'CALL_B_DAY_C', 'MORNING', 'AFTERNOON', 'EVENING',\n",
    "       'NIGHT', 'S1', 'S2', 'S3', 'S4', 'YR_DUMMY',\n",
    "       'CALL_A_MORN', 'CALL_A_AFTER', 'CALL_A_EVE', 'CALL_A_NIT',\n",
    "       'CALL_B_MORN', 'CALL_B_AFTER', 'CALL_B_EVE', 'CALL_B_NIT',\n",
    "       'CALL_C_MORN', 'CALL_C_AFTER', 'CALL_C_EVE', 'CALL_C_NIT', 'CALL_A_S1',\n",
    "       'CALL_A_S2', 'CALL_A_S3', 'CALL_A_S4', 'CALL_B_S1', 'CALL_B_S2',\n",
    "       'CALL_B_S3', 'CALL_B_S4', 'CALL_C_S1', 'CALL_C_S2', 'CALL_C_S3',\n",
    "       'CALL_C_S4', 'DAY_A_MORN', 'DAY_A_AFTER', 'DAY_A_EVE', 'DAY_A_NIT',\n",
    "       'DAY_B_MORN', 'DAY_B_AFTER', 'DAY_B_EVE', 'DAY_B_NIT', 'DAY_C_MORN',\n",
    "       'DAY_C_AFTER', 'DAY_C_EVE', 'DAY_C_NIT', 'DAY_A_S1', 'DAY_A_S2',\n",
    "       'DAY_A_S3', 'DAY_A_S4', 'DAY_B_S1', 'DAY_B_S2', 'DAY_B_S3', 'DAY_B_S4',\n",
    "       'DAY_C_S1', 'DAY_C_S2', 'DAY_C_S3', 'DAY_C_S4', 'CALL_A_MISS',\n",
    "       'CALL_B_MISS', 'CALL_C_MISS', 'DAY_A_MISS', 'DAY_B_MISS', 'DAY_C_MISS','TAXI_ID_CAT','ORIGIN_CALL_CAT', 'ORIGIN_STAND_CAT']\n",
    "continous_names = ['HR_SIN', 'HR_COS', 'DAY_SIN','DAY_COS', 'WK_SIN', 'WK_COS', 'MON_SIN', 'MON_COS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = TensorDataset(torch.from_numpy(x_train[category_names].values.astype(np.compat.long)),\n",
    "                                            torch.from_numpy(x_train[continous_names].values.astype(np.float32)),\n",
    "                                            torch.from_numpy(y_train.values)), \\\n",
    "                              TensorDataset(torch.from_numpy(x_test[category_names].values.astype(np.compat.long)),\n",
    "                                            torch.from_numpy(x_test[continous_names].values.astype(np.float32)),\n",
    "                                            torch.from_numpy(y_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True), \\\n",
    "                                                      DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(new_data['ORIGIN_STAND_CAT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tab_transformer_pytorch import FTTransformer\n",
    "\n",
    "\n",
    "categoryies =  [2] * (len(category_names)-3) + [448] + [57106] + [64]\n",
    "# categoryies.insert(0,448)\n",
    "num_continuous = len(continous_names)\n",
    "\n",
    "model = FTTransformer(\n",
    "    categories = tuple(categoryies),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = num_continuous,                # number of continuous values\n",
    "    dim = 128,                           # dimension, paper set at 32\n",
    "    dim_out = 1,                        # binary prediction, but could be anything\n",
    "    depth = 10,                          # depth, paper recommended 6\n",
    "    heads = 8,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.2,                 # post-attention dropout\n",
    "    ff_dropout = 0.2                    # feed forward dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,lr_scheduler,train_loader,val_loader,epochs,output_path,device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    best_loss = np.inf\n",
    "    with tqdm(total=epochs, desc=f'Training', postfix=dict, mininterval=0.3) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            train_total_loss = []\n",
    "            model.train()\n",
    "            for i,(x,x1,label) in enumerate(train_loader):\n",
    "                x = x.to(device)\n",
    "                x1 = x1.to(device)\n",
    "                label = label.to(device)\n",
    "                model_output = model(x,x1).squeeze(1)\n",
    "\n",
    "                loss = loss_fn(model_output,label.float())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5, norm_type=2)\n",
    "                optimizer.step()\n",
    "                train_total_loss.append(loss.item())\n",
    "\n",
    "            avg_train_loss = np.mean(train_total_loss)\n",
    "\n",
    "\n",
    "            val_total_loss = []\n",
    "            model.eval()\n",
    "            for i,(x,x1,label) in enumerate(val_loader):\n",
    "                x = x.to(device)\n",
    "                x1 = x1.to(device)\n",
    "                label = label.to(device)\n",
    "                model_output = model(x,x1).squeeze(1)\n",
    "                loss = loss_fn(model_output,label.float())\n",
    "\n",
    "\n",
    "                val_total_loss.append(loss.item())\n",
    "\n",
    "\n",
    "            avg_val_loss = np.mean(val_total_loss)\n",
    "\n",
    "\n",
    "            pbar.set_postfix(**{'train_rmse_loss': round(np.sqrt(avg_train_loss),5),\n",
    "                                'val_rmse_loss': round(np.sqrt(avg_val_loss),5)})\n",
    "            pbar.update(1)\n",
    "\n",
    "            if best_loss > avg_val_loss:\n",
    "                best_loss = avg_val_loss\n",
    "                save_name = os.path.join(output_path,f\"Taxi_Epoch{epoch}_rmse{round(np.sqrt(avg_val_loss),5)}_withid.pth\")\n",
    "                torch.save(model.state_dict(),save_name)\n",
    "\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add encountered 22 time(s)\n",
      "Unsupported operator aten::embedding encountered 1 time(s)\n",
      "Unsupported operator aten::mul encountered 145 time(s)\n",
      "Unsupported operator aten::softmax encountered 10 time(s)\n",
      "Unsupported operator aten::gelu encountered 10 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs:  257.677421312\n",
      "| name                         | #elements or shape   |\n",
      "|:-----------------------------|:---------------------|\n",
      "| model                        | 10.0M                |\n",
      "|  cls_token                   |  (1, 1, 128)         |\n",
      "|  categorical_embeds          |  7.4M                |\n",
      "|   categorical_embeds.weight  |   (57772, 128)       |\n",
      "|  numerical_embedder          |  2.0K                |\n",
      "|   numerical_embedder.weights |   (8, 128)           |\n",
      "|   numerical_embedder.biases  |   (8, 128)           |\n",
      "|  transformer                 |  2.6M                |\n",
      "|   transformer.layers         |   2.6M               |\n",
      "|    transformer.layers.0      |    0.3M              |\n",
      "|    transformer.layers.1      |    0.3M              |\n",
      "|    transformer.layers.2      |    0.3M              |\n",
      "|    transformer.layers.3      |    0.3M              |\n",
      "|    transformer.layers.4      |    0.3M              |\n",
      "|    transformer.layers.5      |    0.3M              |\n",
      "|    transformer.layers.6      |    0.3M              |\n",
      "|    transformer.layers.7      |    0.3M              |\n",
      "|    transformer.layers.8      |    0.3M              |\n",
      "|    transformer.layers.9      |    0.3M              |\n",
      "|  to_logits                   |  0.4K                |\n",
      "|   to_logits.0                |   0.3K               |\n",
      "|    to_logits.0.weight        |    (128,)            |\n",
      "|    to_logits.0.bias          |    (128,)            |\n",
      "|   to_logits.2                |   0.1K               |\n",
      "|    to_logits.2.weight        |    (1, 128)          |\n",
      "|    to_logits.2.bias          |    (1,)              |\n"
     ]
    }
   ],
   "source": [
    "from fvcore.nn import FlopCountAnalysis, parameter_count_table\n",
    "\n",
    "categories =  [2] * (len(category_names)-3) + [448] + [57106] + [64]\n",
    "num_continuous = len(continous_names)\n",
    "model.cpu()\n",
    "for x,x1,y in train_dataloader:\n",
    "    dummy_input = (x,x1)\n",
    "    break\n",
    "\n",
    "# FLOPs\n",
    "flops = FlopCountAnalysis(model, dummy_input)\n",
    "print(\"FLOPs: \", flops.total()/1e9)\n",
    "\n",
    "# parameters\n",
    "print(parameter_count_table(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 123/500 [13:45:44<42:10:56, 402.80s/it, train_rmse_loss=658, val_rmse_loss=671]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/data/aaron/Homework/Taxi/tabtransformer.ipynb 单元格 19\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m/data/aaron/Homework/Taxi/model/Taxi_Epoch191_rmse662.93517_withid.pth\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(ckpt)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m train(model,optimizer,lr_scheduler,train_dataloader,test_dataloader,epochs,output_path,device)\n",
      "\u001b[1;32m/data/aaron/Homework/Taxi/tabtransformer.ipynb 单元格 19\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(model_output,label\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=5, norm_type=2)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244656c6c54363430227d/data/aaron/Homework/Taxi/tabtransformer.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/cv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1,weight_decay=5e-4)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.92)\n",
    "\n",
    "epochs = 500\n",
    "output_path = \"./model/\"\n",
    "\n",
    "resume = False\n",
    "if resume:\n",
    "    ckpt = torch.load(\"/data/aaron/Homework/Taxi/model/Taxi_Epoch191_rmse662.93517_withid.pth\")\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "train(model,optimizer,lr_scheduler,train_dataloader,test_dataloader,epochs,output_path,device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(\"test_public_features_cat.csv\")\n",
    "new_data.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "\n",
    "public_test_dataset = TensorDataset(torch.from_numpy(new_data[category_names].values.astype(np.int8)),\n",
    "                                            torch.from_numpy(new_data[continous_names].values.astype(np.float32)))\n",
    "\n",
    "public_test_dataloader = DataLoader(public_test_dataset, batch_size=320, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"/data/aaron/Homework/Taxi/model/Taxi_Epoch104_rmse660.11802_withid.pth\")\n",
    "model.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the results\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i,(x,x1) in enumerate(public_test_dataloader):\n",
    "        x = x.to(device)\n",
    "        x1 = x1.to(device)\n",
    "        output = model(x,x1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample\n",
    "\n",
    "sample = pd.read_csv(\"/data/aaron/Homework/Taxi/sample_xgboost.csv\",index_col=\"TRIP_ID\")\n",
    "# sample.drop([\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "\n",
    "sample['TRAVEL_TIME'] = output.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(\"sample_transformer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
